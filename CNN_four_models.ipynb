{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_img = np.zeros((27548, 224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimage import io, color, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27548, 224, 224, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "1000\n",
      "3000\n",
      "5000\n",
      "7000\n",
      "10000\n",
      "13000\n",
      "15000\n",
      "17000\n",
      "20000\n",
      "22000\n",
      "(27548, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bigbasket_products.csv', engine='python', encoding='utf-8', error_bad_lines=False)\n",
    "imgurl_list = df['image_url'].tolist()\n",
    "# category_list = df['category'].tolist()\n",
    "\n",
    "# cat_LE = LabelEncoder().fit_transform(category_list)\n",
    "# cat_OneHot = np_utils.to_categorical(cat_LE) \n",
    "\n",
    "# preprocessed_img = []\n",
    "i = 0\n",
    "idx = 0\n",
    "err = []\n",
    "arr = [100, 1000, 3000, 5000, 7000, 10000, 13000, 15000, 17000, 20000, 22000]\n",
    "for url in imgurl_list:\n",
    "\n",
    "    if i in arr:\n",
    "        print(i)\n",
    "        \n",
    "    try:\n",
    "        img = io.imread(url)\n",
    "        # img = img.astype('uint8')\n",
    "        # print('1', img)\n",
    "\n",
    "        # grayscale conversion\n",
    "        # img_gray = color.rgb2gray(img)\n",
    "\n",
    "        # standardization: scales and preprocesses images to have similar heights and widths => goal: standard deviation = 1(unit variance), mean = 0\n",
    "        img_resized = transform.resize(img, (224, 224))\n",
    "        # img_resized = img_resized.astype('uint8')\n",
    "        # print('2', img_resized)\n",
    "\n",
    "        # normalization: projecting image data pixels (intensity) to a predefined range (usually (0,1) or (-1, 1))\n",
    "        img_resized_norm = (img_resized - np.min(img_resized)) / (np.max(img_resized) - np.min(img_resized))\n",
    "        # img_resized_norm = img_resized_norm.astype('uint8')\n",
    "        # print('3', img_resized_norm.shape)\n",
    "\n",
    "        np_img[idx] = img_resized_norm\n",
    "        idx += 1\n",
    "\n",
    "        # preprocessed_img.append(img_resized_norm)\n",
    "        del img\n",
    "        del img_resized\n",
    "        del img_resized_norm\n",
    "    except:\n",
    "        err.append(i)\n",
    "        continue\n",
    "    i += 1\n",
    "\n",
    "# preprocessed_img = np.asarray(preprocessed_img)\n",
    "# print(preprocessed_img.shape)\n",
    "# plt.imshow(preprocessed_img[0,:,:])\n",
    "# plt.show() \n",
    "print(np_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X.npy', np_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras import datasets\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from skimage import io, color, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# https://www.section.io/engineering-education/image-preprocessing-in-python/\n",
    "df = pd.read_csv('amazon_data.csv', engine='python', encoding='utf-8', error_bad_lines=False, nrows =100)\n",
    "imgurl_list = df['image url'].tolist()\n",
    "category_list = df['category'].tolist()\n",
    "\n",
    "# one hot encoding\n",
    "cat_LE = LabelEncoder().fit_transform(category_list)\n",
    "cat_OneHot = np_utils.to_categorical(cat_LE) \n",
    "\n",
    "# list of numpy array\n",
    "preprocessed_img = []\n",
    "for url in imgurl_list:\n",
    "    # io.imread(): returns a numpy array\n",
    "    img = io.imread(url)\n",
    "    img = img.astype('uint8')\n",
    "\n",
    "    # grayscale conversion\n",
    "#     img_gray = color.rgb2gray(img)\n",
    "    # standardization: scales and preprocesses images to have similar heights and widths => goal: standard deviation = 1(unit variance), mean = 0\n",
    "    img_resized = transform.resize(img, (224, 224))\n",
    "    img_resized = img_resized.astype('uint8')\n",
    "    # normalization: projecting image data pixels (intensity) to a predefined range (usually (0,1) or (-1, 1))\n",
    "    img_resized_norm = (img_resized - np.min(img_resized)) / (np.max(img_resized) - np.min(img_resized))\n",
    "    img_resized_norm = img_resized_norm.astype('uint8')\n",
    "    \n",
    "    del img\n",
    "    \n",
    "    preprocessed_img.append(img_resized_norm)\n",
    "\n",
    "preprocessed_img = np.asarray(preprocessed_img)\n",
    "print(preprocessed_img.shape)\n",
    "plt.imshow(preprocessed_img[2,:,:])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(preprocessed_img, cat_OneHot, test_size=0.2,shuffle = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_img, cat_LE, test_size=0.2,shuffle = True)\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "classes = len(set(cat_LE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "# X_train = tf.image.resize(X_train, [224, 224])\n",
    "# X_test = tf.image.resize(X_test, [224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# a = tf.constant([[1, 2], [3, 4]])                 \n",
    "# b = tf.add(a, 1)\n",
    "\n",
    "# a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 224, 224, 3)\n",
      "(80,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = len(np.unique(y_test))\n",
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, concatenate, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "import numpy as np\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def Conv2d_BN(x, nb_filter, kernel_size, padding='same', strides=(1, 1), name=None):\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "\n",
    "    x = Conv2D(nb_filter, kernel_size, padding=padding, strides=strides, activation='relu', name=conv_name)(x)\n",
    "    x = BatchNormalization(axis=3, name=bn_name)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def Inception(x, nb_filter):\n",
    "    branch1x1 = Conv2d_BN(x, nb_filter, (1, 1), padding='same', strides=(1, 1), name=None)\n",
    "\n",
    "    branch3x3 = Conv2d_BN(x, nb_filter, (1, 1), padding='same', strides=(1, 1), name=None)\n",
    "    branch3x3 = Conv2d_BN(branch3x3, nb_filter, (3, 3), padding='same', strides=(1, 1), name=None)\n",
    "\n",
    "    branch5x5 = Conv2d_BN(x, nb_filter, (1, 1), padding='same', strides=(1, 1), name=None)\n",
    "    branch5x5 = Conv2d_BN(branch5x5, nb_filter, (1, 1), padding='same', strides=(1, 1), name=None)\n",
    "\n",
    "    branchpool = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branchpool = Conv2d_BN(branchpool, nb_filter, (1, 1), padding='same', strides=(1, 1), name=None)\n",
    "\n",
    "    x = concatenate([branch1x1, branch3x3, branch5x5, branchpool], axis=3)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "inpt = Input(shape=(224, 224, 3))\n",
    "# padding = 'same'，填充為(步長-1）/2,還可以用ZeroPadding2D((3,3))\n",
    "x = Conv2d_BN(inpt, 64, (7, 7), strides=(2, 2), padding='same')\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = Conv2d_BN(x, 192, (3, 3), strides=(1, 1), padding='same')\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = Inception(x, 64)  # 256\n",
    "x = Inception(x, 120)  # 480\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = Inception(x, 128)  # 512\n",
    "x = Inception(x, 128)\n",
    "x = Inception(x, 128)\n",
    "x = Inception(x, 132)  # 528\n",
    "x = Inception(x, 208)  # 832\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = Inception(x, 208)\n",
    "x = Inception(x, 256)  # 1024\n",
    "x = AveragePooling2D(pool_size=(7, 7), strides=(7, 7), padding='same')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Flatten(x)\n",
    "x = Dense(1000, activation='relu')(x)\n",
    "x = Dense(classes, activation='softmax')(x)\n",
    "model = Model(inpt, x, name='inception')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # 注意!loss有時候是sparse_\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\c5500\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 224, 224, 10) 40          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 224, 224, 10) 40          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 224, 224, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 224, 224, 10) 910         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 224, 224, 10) 2510        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 224, 224, 10) 40          max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 224, 224, 30) 0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1505280)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1200)         1806337200  flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 600)          720600      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 150)          90150       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 22)           3322        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,807,154,812\n",
      "Trainable params: 1,807,154,812\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras import models\n",
    "from keras.models import Model\n",
    "\n",
    "input_img = Input(shape=(224, 224, 3))\n",
    "\n",
    "### 1st layer\n",
    "layer_1 = Conv2D(10, (1,1), padding='same', activation='relu')(input_img)\n",
    "layer_1 = Conv2D(10, (3,3), padding='same', activation='relu')(layer_1)\n",
    "\n",
    "layer_2 = Conv2D(10, (1,1), padding='same', activation='relu')(input_img)\n",
    "layer_2 = Conv2D(10, (5,5), padding='same', activation='relu')(layer_2)\n",
    "\n",
    "layer_3 = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_img)\n",
    "layer_3 = Conv2D(10, (1,1), padding='same', activation='relu')(layer_3)\n",
    "\n",
    "mid_1 = keras.layers.concatenate([layer_1, layer_2, layer_3], axis = 3)\n",
    "\n",
    "flat_1 = Flatten()(mid_1)\n",
    "\n",
    "dense_1 = Dense(1200, activation='relu')(flat_1)\n",
    "dense_2 = Dense(600, activation='relu')(dense_1)\n",
    "dense_3 = Dense(150, activation='relu')(dense_2)\n",
    "output = Dense(classes, activation='softmax')(dense_3)\n",
    "\n",
    "model = Model(input_img, output)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # 注意!loss有時候是sparse_\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples, validate on 16 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1505280,1200] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node training/Adam/mul_51}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0bdac2df49ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1505280,1200] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node training/Adam/mul_51}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
